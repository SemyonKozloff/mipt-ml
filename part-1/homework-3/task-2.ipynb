{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E_{x,y}E_{X^l} \\left[\\left(y - a_{X^l}(x)\\right)^2\\right] = E_{x,y}E_{X^l}\\left[\\left(y - E_{X^l}a_{X^l}(x) + E_{X^l}a_{X^l}(x) - a_{X^l} \\right)^2\\right] = E_{X^l}a_{X^l}\\left[\\left(y - E_{X^l}a_{X^l}(x)\\right)\\right] + 2\\cdot E_{x,y}E_{X^l}\\left[\\left(y - E_{X^l}a_{X^l}(x)\\right)\\cdot\\left(E_{X^l}a_{X^l}(x)- a_{X^l}(x)\\right)\\right] + E_{x,y}E_{X^l}\\left[\\left(E_{x^l}a_{X^l}(x) - a_{X^l}(x)\\right)^2\\right]$\n",
    "\n",
    "Посчитаем 2-ое слагаемое:\n",
    "\n",
    "$2\\cdot E_{x,y}E_{X^l}\\left[\\left(y - E_{X^l}a_{X^l}(x)\\right)\\cdot\\left(E_{X^l}a_{X^l}(x)- a_{X^l}(x)\\right)\\right] = $ $ = 2\\cdot \\left(E_{x,y}E_{X^l}\\left[y\\cdot E_{X^l}a_{X^l}(x) - E_{x,y}(E_{X^l}a_{X^l}(x))^2\\right] - E_{x,y}\\left[(y\\cdot E_{X^l}a_{X^l}(x) + E_{x,y}(E_{X^l}a_{X^l}(x))^2\\right] \\right) = 0$\n",
    "\n",
    "Тогда:\n",
    "\n",
    "$E_{x,y}E_{X^l}\\left[\\left(y - a_{X^l}(x)\\right)^2\\right] = E_{x,y}E_{X^l}\\left[\\left(y - E_{X^l}a_{X^l}(x)\\right)^2 \\right] + Var = E_{x,y}E_{X^l}\\left[\\left(y - E(y|x) + E(y|x) + a_{X^l}(x)\\right)^2\\right] + Var = $ $ = E_{x,y}\\left[\\left(y - E(y|x)\\right)^2\\right] + E_{x,y}\\left[\\left(E(y|x) - E_{X^l}a_{X^l}(x)\\right)^2\\right] + 2 \\cdot E_{x,y}\\left[(y - E(y|x))(E(y|x) - E_{X^l}a_{X^l}(x))\\right] + Var$\n",
    "\n",
    "Теперь воспользуемся независимостью и $E(E(y|x) - E_{X^l}a_{X^l}(x)) = 0$\n",
    "\n",
    "$2\\cdot E_{x,y}\\left[ \\left( y - E(y|x) \\right) \\cdot\\left(E(y|x) - E_{X^l}a_{X^l}(x)\\right)\\right] = 2 \\cdot E_{x,y}(y-E(y|x))\\cdot E_{x,y}(E(y|x)-E_{X^l}a_{x^l}(x)) = 0$\n",
    "\n",
    "То есть в конечном итоге мы получаем:\n",
    "\n",
    "$E_{x,y}E_{X^l}(y - a_{X^l}(x))^2 = Noise + Bias^2 + Var$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$a(x)=\\frac{1}{M}\\sum_{m=1}^M a_m (x)$, отсюда $E_x a(x)=E_x a_1 (x)$ в силу линейности.\n",
    "\n",
    "Так как $Bias=E_{x, y}[E(y|x)-E_x a(x)]^2$, то очевидно, что Bias для композиции и базового алгоритма одинаковый.\n",
    "\n",
    "Посмотрим теперь на разброс.\n",
    "\n",
    "$Var=E[\\frac{1}{M}\\sum_1^M a_m (x)-ME_x a_1 (x)]^2=\\frac{1}{M^2}E_{x,y}E_{x}[\\sum_{m=1}^{M}(a_m(x)-E_x a_1(x)]^2=$ $=\\frac{1}{M^2} E_{x,y}E_x \\sum_{m=1}^M (a_m (x)-E_x a_1 (x))^2+$ $+\\frac{1}{M^2}E_{x,y}E_x\\sum_{m\\neq k}(a_m(x)-Ea_1(x))(a_k(x)-Ea_1(x))=$ $=\\frac{1}{M}Var[a_1(x)]+\\frac{1}{M^2}\\sum_{m\\neq k} cov(a_m(x), a_k(x))$\n",
    "\n",
    "Если считать, что $cov(a_m(x), a_k(x)) = 0$, то разброс при использовании бэггинга уменьшится в $M$ раз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть $Var\\xi_1 = \\sigma^2$. Если $corr(\\xi_i, \\xi_j)=\\rho$, то $cov(\\xi_i, \\xi_j)=\\rho\\sigma^2$.\n",
    "\n",
    "Воспользуемся формулой из предыдущей задачи. \n",
    "\n",
    "$Var[\\frac{1}{M}\\sum_{i=1}^{M}\\xi_i]=\\frac{1}{M}\\sigma^2+\\frac{M-1}{M}\\rho\\sigma^2=\\rho\\sigma^2+(1-\\rho)\\frac{\\sigma^2}{M}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
